# 2021年自然语言处理\_期末真题_回忆版

> 【author】Maxpicca-Li
>
> 【写在前面】
>
> 1. 考试时间：2021年12月24日14:00~16:00。
>
> 2. 试卷整体简单，开卷，复习的时候还是把书上的一些重点看一下，要不然都不能**秒翻**到内容。不能**秒翻**到内容的话，就可能面临着抄书抄不完的情况。考试感慨：NLP开卷考试=抄书+手撕CNN和HMM。
>
> 3. 本回忆版真题于2021年12月24日19点20分写成，因为情绪不好，不想和朋友说话。
>
> 4. 计院专业课的试卷似乎都不准老师发出来, 希望有学弟学妹们能将我”回 忆试卷”的习惯传承下去! 
>    ——from VayneDuan
>    
> 5. 其余专业课的回忆版试卷如下：记得 star & follow, 会持续更新的!
>    
>    Source：[VayneDuan/cqu-cs-learning-materials](https://github.com/VayneDuan/cqu-cs-learning-materials)
>    
>    Fork：[Maxpicca-Li/cqu-cs-learning-materials](https://github.com/Maxpicca-Li/cqu-cs-learning-materials)
>
> <p style="text-align:right;">——【写在前面】基于原作者VayneDuan修改</p>

## 填空题（10道*2分=20分）

1. **命名实体识别**的定义
2. LSTM中，**输出门** 用于表示到下一个细胞状态的信息
3. $p_1=1/2, p_2=1/4, p_3=1/8, p_4=1/8$，求熵：$7/4$
4. 无向图中，不能使用条件概率密度作为参数化，应该使用 **联合概率密度** 的参数化
5. WordNet中，“猫”和“哺乳动物”属于 **上下位关系**
6. ”他将来学计算机”，属于**组合型歧义**
7. **余弦**函数用于评估两个向量之间的相似度。
8. 文本分类性能评价中，**召回率 **表示分类正确的个数在所有标准答案中的比例。
9. 。。。
10. 。。。

## 简答题（4道*5分=20分）

1. 自然语言处理的概念、主要研究内容和方法
2. 最大似然估计的基本原理
3. 交叉熵的概念、物理意义和应用
4. 请阐述语言知识库与典型语言知识库

## 计算题（2道*15分=30分）

1. CNN相关（考的很简单）
   1. 步长为1无pad的卷积
   2. 步长为2有外围0 pad的卷积
   3. 平均池化
   4. relu激活函数
2. HMM相关
   1. viterbi算法，前向递推两个时刻
   2. 回溯，计算最优隐状态序列

## **综合题（3道*10分=30分）**

1. 列举文本分类中特征选择的几种方法，并分析其特点、存在的问题及解决方法

 CHI算法的缺点：

  1、词频问题。CHI算法仅考虑特征词是否出现在文档中，没有考虑特征词的出现次数。原始CHI公式中，A、B、C、D计算的都是特征是否在文本中出现，而不是特征在文本中出现的几次，相当于求布尔值。

  2、文档分布。对于具体类别内部，若某特征词仅在此类中的个别文档中出现，在其它文档中不出现，则该特征词所代表的类别信息就少很多。

  3、类别频。若特征词只出现在极少类别中，它就会比在较多类别中都出现的特征更重要。

  4、负相关特性。负相关特性会对特征的重要性产生负面的影响，从CHI的原始公式中可以看出，特征词与类别成正相关，即Ａ×Ｄ－Ｂ×Ｃ＞０，卡方值越大，特征越重要。若特征词与类别成负相关，即Ａ×Ｄ－Ｂ×Ｃ ＜０，该特征词对此类别越不重要，卡方值应越小，但实际卡方值却变大，这就影响了分类效果。

基于互信息算法的缺点：

这个计算是以doc为单位的，只考虑doc是否包含w，doc是否属于c，而不管w在doc中的出现次数，这是第一个缺点。

此外根据互信息公式，p(w)在分母中，因此很容易选出小众词，而小众词是否是很强的特征呢？如果是非常专业的文章(如医学论文)，那么小众词可能是的，对于普通分类（如娱乐、体育等），小众词可能就是一些干扰因素，选出太多的小众词作为特征是不合适的。这是第二个缺点。



2. 对两种向量表示方法——向量空间模型vsm和word2vec进行综合分析并对比

向量空间模型（VSM）表示（嵌入）连续向量空间中的单词，其中语义上相似的单词被映射到相邻的点（「都嵌在彼此附近」）。向量空间模型在自然语言处理中有着悠久、丰富的历史，但是所有方法都以某种方式依赖于分布假说，该假说认为出现在相同语境中的词语具有相似的语义。基于这一原则的方法可以被分为两类：

1. 基于计数的方法（例如隐性语义分析）

2. 预测方法（例如神经概率语言模型）

Word2vec 是一种用于从原始文本中学习词嵌入的模型，它有很高的计算效率。它主要有两种实现方式，一种是连续词袋模型（CBOW），另一种是 Skip-Gram 模型。这两种方式在算法上是相似的，唯一的差别在于 CBOW 从源上下文单词中预测目标单词，而 Skip-Gram 则恰恰相反，它根据目标单词预测源上下文单词。

1. **命名实体识别中的基于多特征的识别方法，请用该方法对人名进行实体识别，并写出具体的步骤（模型以及模型参数）**

